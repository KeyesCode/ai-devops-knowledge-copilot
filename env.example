DB_USER=postgres
DB_PASSWORD=postgres
DB_NAME=knowledge_copilot
DB_HOST=localhost
DB_PORT=5432

REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_ENABLED=true

# Backend
PORT=3000

# Embedding Service
# Provider options: 'openai' or 'ollama'
EMBEDDING_PROVIDER=openai
EMBEDDING_CACHE_ENABLED=true
EMBEDDING_CACHE_TTL=86400

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Ollama (used when EMBEDDING_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# LLM Service
# Provider options: 'openai' or 'ollama'
LLM_PROVIDER=openai

# OpenAI LLM (used when LLM_PROVIDER=openai)
OPENAI_LLM_MODEL=gpt-4o-mini

# Ollama LLM (used when LLM_PROVIDER=ollama)
OLLAMA_LLM_MODEL=llama3.2

# GitHub API (optional, but recommended to avoid rate limits)
GITHUB_TOKEN=your_github_token_here

# Reranker Service (Cross-Encoder)
# Enable/disable cross-encoder reranking for improved retrieval accuracy
# Note: Reranking is optional. If the model fails to load, retrieval will continue without reranking.
RERANKER_ENABLED=true
# Model to use for reranking (default: Xenova/ms-marco-MiniLM-L-6-v2)
# Must use Xenova/* repos which include ONNX files (not PyTorch cross-encoder/* repos)
# Alternative models: Xenova/ms-marco-MiniLM-L-12-v2 (larger, more accurate)
RERANKER_MODEL=Xenova/ms-marco-MiniLM-L-6-v2
# Number of top results to return after reranking
RERANKER_TOP_K=20
# Backend preference: 'wasm' (most stable), 'node' (faster if supported), or 'auto' (default)
# WASM is slower but works across all platforms. Node backend is faster but may have compatibility issues.
RERANKER_BACKEND=auto
# Quantization: 'true' (try quantized first), 'false' (non-quantized only), or 'auto' (default, tries both)
# Quantized models are smaller and faster but may fail on some platforms
RERANKER_QUANTIZED=auto

# JWT Authentication
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production
JWT_EXPIRES_IN=7d

